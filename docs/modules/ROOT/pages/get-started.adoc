= Deploy a Cluster with the Hazelcast Platform Operator for Kubernetes
:description: In this tutorial, you'll deploy a Hazelcast cluster and an instance of Management Center, using Hazelcast Platform Operator for Kubernetes.

{description}

== Before you Begin

You need a Kubernetes or Openshift cluster, and the `kubectl` or `oc` command-line tool must be configured to communicate with your cluster.

== Step 1. Deploy Hazelcast Platform Operator

From the Hazelcast Platform Operator 5.6.0 onwards, you can use a Helm chart to install the Hazelcast Platform Operator.

. Add the link:https://github.com/hazelcast/charts[Hazelcast Helm Charts repository] to your Helm repository list by running the following command:

+
[source,shell]
----
helm repo add hazelcast https://hazelcast-charts.s3.amazonaws.com/
helm repo update
----

. You can either deploy the Hazelcast Platform Operator at the same time as CRDs or separately.

+
NOTE: Since CRDs are global resources, they may need to be installed by an administrator.

.. Run the following command to *deploy the operator and the CRDs together*.

+
[source,shell,subs="attributes"]
----
helm install operator hazelcast/hazelcast-platform-operator --version={operator-chart-version} --set installCRDs=true
----

.. Run the following commands to *deploy the operator and the CRDs separately*.

+
[source,shell,subs="attributes"]
----
helm install operator-crds hazelcast/hazelcast-platform-operator-crds --version={operator-chart-version}
----

+
After installing CRDs, install the operator by running the following command:

+
[source,shell,subs="attributes"]
----
helm install operator hazelcast/hazelcast-platform-operator --version={operator-chart-version}
----

+
. Monitor the operator logs. At this point, the Hazelcast Platform Operator should be up and running. You can check it with the command below.
+
[tabs]
====
Kubernetes::
+
--
[source,shell]
----
kubectl logs deployment.apps/operator-hazelcast-platform-operator
----
--
Openshift::
+
--
[source,shell]
----
oc logs deployment.apps/operator-hazelcast-platform-operator
----
--
====

== Step 2. Start the Hazelcast Cluster

After installing and running the Hazelcast Platform Operator, you can create a Hazelcast cluster. First, create the `Hazelcast` custom resource file as `hazelcast.yaml`.

[tabs]
====
Open Source::
+
--

. Create the `Hazelcast` custom resource file and name it `hazelcast.yaml`.
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast.yaml[]
----

. Apply the custom resource to start the Hazelcast cluster.
+

.For Kubernetes
[source,shell]
----
kubectl apply -f hazelcast.yaml

----
+
.For Openshift
[source,shell]
----
oc apply -f hazelcast.yaml
----


. Verify that the cluster is up and running by checking the Hazelcast member logs.
+

.For Kubernetes
[source,shell]
----
kubectl logs pod/hazelcast-sample-0
----
+
.For Openshift
[source,shell]
----
oc logs pod/hazelcast-sample-0
----

You should see the following:

```
Members {size:3, ver:3} [
        Member [10.36.8.3]:5701 - ccf31703-de3b-4094-9faf-7b5d0dc145b2 this
        Member [10.36.7.2]:5701 - e75bd6e2-de4b-4360-8113-040773d858b7
        Member [10.36.6.2]:5701 - c3d105d2-0bca-4a66-8519-1cacffc05c98
]
```
--
Enterprise::
+
--
Hazelcast Enterprise requires a license key. If you don't have a license key, you can request one from the link:http://trialrequest.hazelcast.com/[Hazelcast website].

. Create a Kubernetes secret to hold your license key.
+
.For Kubernetes
[source,shell]
----
kubectl create secret generic hazelcast-license-key --from-literal=license-key=<YOUR LICENSE KEY>
----
+
.For Openshift
[source,shell]
----
oc create secret generic hazelcast-license-key --from-literal=license-key=<YOUR LICENSE KEY>
----

. Create the `Hazelcast` custom resource file and name it `hazelcast-enterprise.yaml`.
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-enterprise.yaml[]
----

. Apply the custom resource to start the Hazelcast cluster.
+
.For Kubernetes
[source,shell]
----
kubectl apply -f hazelcast-enterprise.yaml
----
+
.For Openshift
[source,shell]
----
oc apply -f hazelcast-enterprise.yaml
----

. Verify that Hazelcast cluster is up and running by checking the Hazelcast member logs.
+
.For Kubernetes
[source,shell]
----
kubectl logs pod/hazelcast-sample-0
----
+
.For Openshift
[source,shell]
----
oc logs pod/hazelcast-sample-0
----
You should see the following:

```
Members {size:3, ver:3} [
        Member [10.36.8.3]:5701 - ccf31703-de3b-4094-9faf-7b5d0dc145b2 this
        Member [10.36.7.2]:5701 - e75bd6e2-de4b-4360-8113-040773d858b7
        Member [10.36.6.2]:5701 - c3d105d2-0bca-4a66-8519-1cacffc05c98
]
```

--
====

== Step 3. Check that the Hazelcast Cluster is Running

To check if a cluster is running, see the `status` field of the Hazelcast resource.

The status can be checked using the `get hazelcast` command.

[tabs]
====
Kubernetes::
+
--
[source,shell]
----
kubectl get hazelcast
----
--
Openshift::
+
--
[source,shell]
----
oc get hazelcast
----
--
====

```
NAME               STATUS    MEMBERS   EXTERNAL-ADDRESSES
hazelcast-sample   Running   3/3
```

You can use the following command for the long format.


[tabs]
====
Kubernetes::
+
--
[source,shell]
----
kubectl get hazelcast hazelcast-sample -o=yaml
----
--
Openshift::
+
--
[source,shell]
----
oc get hazelcast hazelcast-sample -o=yaml
----
--
====

[source,yaml,subs="attributes+"]
----
status:
  hazelcastClusterStatus:
    readyMembers: 3/3
  phase: Running
----

The `phase` field represents the current status of the cluster, and can contain any of the following values:

* `Running`: The cluster is up and running.
* `Pending`: The cluster is in the process of starting.
* `Failed`: An error occurred while starting the cluster.

Any additional information such as validation errors will be provided in the `message` field.

The `readyMembers` field represents the number of Hazelcast members that are connected to the cluster.

WARNING: Use the `readyMembers` field only for informational purposes. This field is not always accurate. Some members may have joined or left the cluster since this field was last updated.

== Step 4. Start Management Center

You can monitor the Hazelcast cluster by starting Management Center.

[tabs]
====
Open Source::
+
--

. Create the `ManagementCenter` custom resource file and name it `management-center.yaml`.
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center.yaml[]
----

WARNING: StatefulSet does not support updates to `volumeClaimTemplates` field, so `persistence` field should be set only at the creation of the custom resource. Any update to the `persistence` field will not affect the Management Center.

NOTE: By default, xref:management-center:deploy-manage:configuring.adoc#where-management-center-saves-data[Management Center data] is persisted to the `/data` directory. If you want to use an existing `PersistentVolumeClaim`, set its name in the `.spec.persistence.existingVolumeClaimName`field in the Management Center custom resource.

WARNING: `hazelcastClusters` field does not support deleting clusters from the custom resource. If you want to remove a cluster from the Management Center, you can do it from the Management Center UI.


. Apply it with the following command to start Management Center.
+
.For Kubernetes
[source,shell]
----
kubectl apply -f management-center.yaml
----
+
.For Openshift
[source,shell]
----
oc apply -f management-center.yaml
----


. After a moment, you can verify that Management Center is up and running by checking the Management Center logs.
+
.For Kubernetes
[source,shell]
----
kubectl logs pod/managementcenter-sample-0
----
+
.For Openshift
[source,shell]
----
oc logs pod/ -sample-0
----

```
2021-08-26 15:21:04,842 [ INFO] [MC-Client-dev.lifecycle-1] [c.h.w.s.MCClientManager]: MC Client connected to cluster dev.
2021-08-26 15:21:05,241 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.8.3]:5701 - ccf31703-de3b-4094-9faf-7b5d0dc145b2
2021-08-26 15:21:05,245 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.7.2]:5701 - e75bd6e2-de4b-4360-8113-040773d858b7
2021-08-26 15:21:05,251 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.6.2]:5701 - c3d105d2-0bca-4a66-8519-1cacffc05c98
2021-08-26 15:21:07,234 [ INFO] [main] [c.h.w.Launcher]: Hazelcast Management Center successfully started at http://localhost:8080/
```

--
Enterprise::
+
--
To enable some features or dashboard at Management Center, you need a license key. If you don't have a license key, you can request one from the link:http://trialrequest.hazelcast.com/[Hazelcast website].

. Create a Kubernetes secret to hold your license key.
+
.For Kubernetes
[source,shell]
----
kubectl create secret generic hazelcast-license-key --from-literal=license-key=<YOUR LICENSE KEY>
----
+
.For Openshift
[source,shell]
----
oc create secret generic hazelcast-license-key --from-literal=license-key=<YOUR LICENSE KEY>
----

. Create the `ManagementCenter` custom resource file and name it `management-center.yaml`.
+
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/management-center-with-license-key.yaml[]
----

WARNING: StatefulSet does not support updates to `volumeClaimTemplates` field, so `persistence` field should be set only at the creation of the custom resource. Any update to the `persistence` field will not affect the Management Center.

NOTE: By default, xref:management-center:deploy-manage:configuring.adoc#where-management-center-saves-data[Management Center data] is persisted to the `/data` directory. If you want to use an existing `PersistentVolumeClaim`, set its name in the `.spec.persistence.existingVolumeClaimName`field in the Management Center custom resource.

WARNING: `hazelcastClusters` field does not support deleting clusters from the custom resource. If you want to remove a cluster from the Management Center, you can do it from the Management Center UI.


. Apply it with the following command to start Management Center.
+
.For Kubernetes
[source,shell]
----
kubectl apply -f management-center.yaml
----
+
.For Openshift
[source,shell]
----
oc apply -f management-center.yaml
----


. After a moment, you can verify that Management Center is up and running by checking the Management Center logs.
+
.For Kubernetes
[source,shell]
----
kubectl logs pod/managementcenter-sample-0
----
+
.For Openshift
[source,shell]
----
oc logs pod/managementcenter-sample-0
----

```
2021-08-26 15:21:04,842 [ INFO] [MC-Client-dev.lifecycle-1] [c.h.w.s.MCClientManager]: MC Client connected to cluster dev.
2021-08-26 15:21:05,241 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.8.3]:5701 - ccf31703-de3b-4094-9faf-7b5d0dc145b2
2021-08-26 15:21:05,245 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.7.2]:5701 - e75bd6e2-de4b-4360-8113-040773d858b7
2021-08-26 15:21:05,251 [ INFO] [MC-Client-dev.event-1] [c.h.w.s.MCClientManager]: Started communication with member: Member [10.36.6.2]:5701 - c3d105d2-0bca-4a66-8519-1cacffc05c98
2021-08-26 15:21:07,234 [ INFO] [main] [c.h.w.Launcher]: Hazelcast Management Center successfully started at http://localhost:8080/
```

--
====

To access the Management Center dashboard, open the browser at address `http://$MANCENTER_IP:8080`.

[tabs]
====
Kubernetes::
+
--
[source,shell]
----
MANCENTER_IP=$( kubectl get service managementcenter-sample -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
----
--
Openshift::
+
--
[source,shell]
----
MANCENTER_IP=$( oc get service managementcenter-sample -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
----
--
====

If EXTERNAL-IP of the service is hostname, not IP, you can run command below:

[tabs]
====
Kubernetes::
+
--
[source,shell]
----
MANCENTER_IP=$( kubectl get service managementcenter-sample -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
----
--
Openshift::
+
--
[source,shell]
----
MANCENTER_IP=$( oc get service managementcenter-sample -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
----
--
====


== Step 5. Clean up

You can run the commands below to remove the Hazelcast cluster and Management Center.

[tabs]
====
Kubernetes::
+
--
[source,shell]
----
kubectl delete -f hazelcast.yaml
kubectl delete -f management-center.yaml
----
--
Openshift::
+
--
[source,shell]
----
oc delete -f hazelcast.yaml
oc delete -f management-center.yaml
----
--
====

If you installed Hazelcast Enterprise, run the following commands to remove Hazelcast Enterprise cluster and Hazelcast License Key Secret.

[tabs]
====
Kubernetes::
+
--
[source,shell]
----
kubectl delete -f hazelcast-enterprise.yaml
kubectl delete secret hazelcast-license-key
----
--
Openshift::
+
--
[source,shell]
----
oc delete -f hazelcast-enterprise.yaml
oc delete secret hazelcast-license-key
----
--
====

Finally, run the command below to delete the Hazelcast Platform Operator deployment.

[source,shell]
----
helm uninstall operator
----

If you installed the CRDs separately from the operator, you need to remove them by running the following command:

[source,shell]
----
helm uninstall operator-crds
----

== Next Steps

Learn how to xref:connect-outside-kubernetes.adoc[expose Hazelcast clusters outside Kubernetes] so you can connect external clients to them.
