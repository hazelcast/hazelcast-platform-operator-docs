= Persistence, backup and restore
:description: Persistence allows individual members and whole clusters to recover data by persisting map entries, JCache data, and streaming job snapshots on disk. Members can use persisted data to recover from a planned shutdown (including rolling upgrades), a sudden cluster-wide crash, or a single member failure.

{description}

To learn more about persistence, see xref:hazelcast:storage:persistence.adoc[Persisting data on disk].

Backups can be either of the following:

- Local: Local backups are kept in volume and never moved anywhere.
- External: External backups are moved into buckets provided by the user.

TIP: For a worked example, see the xref:tutorials:operator-tutorial-external-backup-restore.adoc[Restore a cluster from cloud storage tutorial].

== Enable persistence

Enabling Hazelcast persistence requires the following configuration.

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence.yaml[]
----

<1> <<recovery-policy, Cluster recovery policy>>.
<2> Size of the PersistentVolumeClaim (PVC) where Hazelcast data is persisted.
<3> Agent responsible for moving data from the local storage to external buckets.
The agent configuration is optional. If you enable `persistence` and do not pass the agent configuration, Operator
uses the latest agent version that is compatible with its version.

If you want to enable persistence for a map or cache you must also enable it in the Map or Cache CRs by setting `persistenceEnabled: true`. For more information, see xref:map-configuration.adoc#configuring-the-map-resource[Configuring Map] and xref:cache-configuration.adoc#configuring-a-cache-resource[Configuring Cache].

NOTE: Make sure to calculate the total disk space that you will use. The total used disk space may be larger than the size of in-memory data, depending on how many <<local-backup, backups>> you take.

[[trigger-backup]]
== Trigger local backups

You can take local backups using the `HotBackup` custom resource. Local backups are kept in volume and are not moved anywhere.

--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-local.yaml[]
----
--

== Trigger external backups

In some cases, using a PVC is not enough. For example, if you need to move data between two Kubernetes clusters. For these cases, you can use external storage.

When persistence is enabled, Hazelcast pods start with a sidecar agent which uploads the backups into an external bucket you provide. For external storage, AWS S3, GCP Bucket and Azure Blob storage options are supported.

To trigger an external backup, you need to configure a bucket URI and a secret to tell Hazelcast where to store backup data and how to authenticate.

[tabs]
====

AWS::
+
--
External backup in AWS S3:
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-aws.yaml[]
----

NOTE: For further information about accessing resources on different cloud providers, see xref:authorization.adoc[Authorization methods to access cloud provider resources].
--

GCP::
+
--
External backup in GCP Bucket:
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-gcp.yaml[]
----

NOTE: For further information about accessing resources on different cloud providers, see xref:authorization.adoc[Authorization methods to access cloud provider resources].
--

Azure::
+
--
External backup in Azure Blob:
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hot-backup-external-azure.yaml[]
----

NOTE: For further information about accessing resources on different cloud providers, see xref:authorization.adoc[Authorization methods to access cloud provider resources].
--
====
<1> The bucket URI where backup data will be stored.
<2> Name of the secret with credentials for accessing the given bucket URI.

== Schedule backups

You can schedule backups using the `schedule` and `hotBackupTemplate` fields of the `CronHotBackup` resource. For more information about the `CronHotBackup` resource, see the xref:api-ref.adoc#cronhotbackupspec[API reference].

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/cron-hot-backup.yaml[]
----

The `schedule` field takes a valid cron expression. For example, you can configure the following scheduled backups:

[cols="1,1"]
|===
| ```30 10 * * *```
| At 10:30 AM every day

| ```0, 0, 1,15,25 * *```
| On 1st, 15th, and 25th of each month at midnight

| ```@monthly```
| On the first day of each month at midnight
|===

For a full list of supported expressions, see the link:https://pkg.go.dev/github.com/robfig/cron/v3@v3.0.1#hdr-CRON_Expression_Format[library documentation].

== Check the status of a backup

To check the status of a local backup, run the following command:
[source,bash]
----
kubectl get hotbackup hot-backup
----

The status of the backup is displayed in the output.
[source,bash]
----
NAME         STATUS
hot-backup   Success
----

== Restore from local backups

To restore a cluster from local backups, you can reapply the `Hazelcast` resource, which gives the cluster access to the PVCs that contain the persisted data. This will restore the Hazelcast cluster from existing `hot-restart` folders.

To restore from local backups that you have taken using the `HotBackup` resource, give the `HotBackup` resource name in the restore configuration. For the restore to work correctly, make sure the status of the `HotBackup` resource is `Success`.

When this restore mechanism is used, the Restore Agent container is deployed with the Hazelcast container in the same Pod. The agent starts as an `initContainer` before the Hazelcast container.


[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-cr-name.yaml[]
----

<1> Hazelcast custom resource name for both backup and restore CRs must be the same, otherwise the restore fails.
<2> `HotBackup` resource name used for the restore. The backup folder name will be the name you provide here.
<3> Agent responsible for restoring data from the local storage.
The agent configuration is optional. If you give `restore` under `persistence` and do not pass the agent configuration, Operator
uses the latest agent version that is compatible with its version.

WARNING: You can use a local backup only once to restore a cluster. We recommend you backup externally if you need to persistently restore a backup across the clusters.

== Restore from external backups

To restore a cluster from an external backup, you can either set up the bucket configuration or give the `HotBackup` resource name that you used to trigger the external backup. In either case, the backup is restored from the external bucket.

When this restore mechanism is used, the Restore Agent container is deployed with the Hazelcast container in the same Pod. The agent starts as an `initContainer` before the Hazelcast container.

NOTE: If you have not created the secret, you must do so in the same way as in <<create-secret, Triggering External Backups>>.

[tabs]
====

Bucket configuration::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-external.yaml[]
----
<1> Bucket URI where backup data will be restored from.
<2> Name of the secret with credentials for accessing the given bucket URI.
<3> Agent which is responsible for restoring data from the external storage.
The agent configuration is optional. If you provide `restore` under `persistence` and do not pass the agent configuration, Operator
uses the latest agent version that is compatible with its version.
--

HotBackup resource name::
+
--
[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-cr-name.yaml[]
----

<1> `HotBackup` resource name used for the restore. The bucket URI and secret are taken from the `HotBackup` resource.
<2> Agent responsible for restoring data from external storage.
The agent configuration is optional. If you provide `restore` under `persistence` and do not pass the agent configuration, Operator
uses the latest agent version that is compatible with its version.
--

====

== Restore from a persistent volume

To restore from local backups that are in an existing link:https://kubernetes.io/docs/concepts/storage/persistent-volumes/[persistent volume], configure `localConfig`.

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-restore-local.yaml[]
----

<1> To successfully attach existing PVCs to the newly created cluster:

- If the older cluster was created by Operator, the name of both the backup and restore Hazelcast custom resources must be the same.
- If the older cluster was created by the Hazelcast Helm Chart, the name of the new CR must be in the following format: `<name>-hazelcast-persistence`. For example, assuming that the older cluster was installed with `helm install hz hazelcast/hazelcast-enterprise`, the new CR name must be `hz-hazelcast-enterprise`.
<2> If you configured maps with persistence, you must create them while creating the Hazelcast CR. To create them while creating the Hazelcast CR, you must use xref:custom-config.adoc[custom configuration]. You cannot apply the Hazelcast CR and then the Map CRs.
<3> `pvcNamePrefix` is the prefix of the existing PVCs. It can be set to `persistence` or `hot-restart-persistence` depending on the installation method of the old cluster. Run the `kubect get pvc` command to decide which setting best suits your requirements.
<4> `baseDir` is the root directory for persistence.
<5> `backupDir` is the directory that contains a backupFolder for each available backup.
<6> `backupFolder` is the directory containing the backup for the restore.

TIP: To find the `backupFolder` value, you can run `kubectl exec -it <hazelcast-custom-resource-name> -c hazelcast -- /bin/bash` to list the contents of your existing installation. If you already deleted your installation, you can run a simple pod that lists the contents of the PVC and checks its logs. This lists the folder structure of the specified PVC. For example:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/pod-local-pvc-content.yaml[]
----
<1> Replace `/data/persistence` with the path to your PV that is mounted inside the container.
<2> Replace `/data/persistence` with the correct `mountPath` specified in the PV.
<3> Replace with the name of the one of the PVCs that is mounted to the cluster from which the backup is taken.

NOTE: Agent copies the backup to be restored from `\{baseDir}/\{backupDir}/\{backupFolder}` to `/data/persistence/base-dir`.

== Configure persistence

=== Data recovery timeout

To choose a data recovery timeout, you can use `dataRecoveryTimeout`. The field takes an integer value representing the timeout in seconds and uses this value to set link:https://docs.hazelcast.com/hazelcast/latest/storage/configuring-persistence#global-persistence-options[validation-timeout-seconds, data-load-timeout-seconds] Hazelcast persistence options.

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-persistence-datarecoverytimeout.yaml[]
----

[[recovery-policy]]
=== Choose a cluster recovery policy

To decide how a cluster should behave when one or more members cannot rejoin after a cluster-wide restart, you can define one of the following cluster recovery policies. Operator supports all the policies in the Hazelcast Platform `cluster-data-recovery-policy configuration` options. For complete descriptions and advice on choosing a policy, see xref:hazelcast:storage:configuring-persistence.adoc#policy[Configure persistence].

[cols="1,1"]
|===
| ```FullRecoveryOnly```
| Does not allow partial start of the cluster.

| ```PartialRecoveryMostRecent```
| Allows partial start with the members that have most recent partition table.

| ```PartialRecoveryMostComplete```
| Allows partial start with the member that have most complete partition table.
|===

=== Configure force or partial start

To recover a cluster that has persistence enabled after a cluster-wide restart, you can force a cluster to delete its persistence stores when one or more members fail to restart by triggering a force start. Alternatively, you can force a cluster to start without some members by triggering a partial start.

[WARNING]
====
* The cluster loses all persisted data after a force start if any member fails to start properly. If all members successfully start, all persisted data remains intact.
* The cluster loses persisted data after a partial start only for specific members that have failed to start properly. Persisted data remains intact for members that successfully start.
====

To trigger the cluster recover action, set the `startupAction`:

[source,yaml,subs="attributes+"]
----
include::ROOT:example$/hazelcast-forcestart.yaml[]
----

`PartialStart` can be triggered only when `clusterDataRecoveryPolicy` is set to `PartialRecoveryMostRecent` or `PartialRecoveryMostComplete`.
